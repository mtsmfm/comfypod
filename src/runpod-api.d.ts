/**
 * This file was auto-generated by openapi-typescript.
 * Do not make direct changes to the file.
 */

export interface paths {
  "/openapi.json": {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * OpenAPI 3.0 schema
     * @description The OpenAPI 3.0 schema.
     */
    get: operations["GetOpenAPI"];
    put?: never;
    post?: never;
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  "/docs": {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * Documentation Page
     * @description Interactive API documentation.
     */
    get: operations["GetDocs"];
    put?: never;
    post?: never;
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  "/pods": {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * List Pods
     * @description Returns a list of Pods.
     */
    get: operations["ListPods"];
    put?: never;
    /**
     * Create a new Pod
     * @description Creates a new [Pod](#/components/schemas/Pod) and optionally deploys it.
     */
    post: operations["CreatePod"];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  "/pods/{podId}": {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * Find a Pod by ID
     * @description Returns a single Pod.
     */
    get: operations["GetPod"];
    put?: never;
    post?: never;
    /**
     * Delete a Pod
     * @description Delete a Pod.
     */
    delete: operations["DeletePod"];
    options?: never;
    head?: never;
    /**
     * Update a Pod
     * @description Update a Pod, potentially triggering a reset.
     */
    patch: operations["UpdatePod"];
    trace?: never;
  };
  "/pods/{podId}/update": {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    /**
     * Update a Pod
     * @description Update a Pod - synonym for PATCH /pods/{podId}.
     */
    post: operations["UpdatePod"];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  "/pods/{podId}/start": {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    /**
     * Start or resume a Pod
     * @description Start or resume a Pod.
     */
    post: operations["StartPod"];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  "/pods/{podId}/stop": {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    /**
     * Stop a Pod
     * @description Stop a Pod.
     */
    post: operations["StopPod"];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  "/pods/{podId}/reset": {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    /**
     * Reset a Pod
     * @description Reset a Pod.
     */
    post: operations["ResetPod"];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  "/pods/{podId}/restart": {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    /**
     * Restart a Pod
     * @description Restart a Pod.
     */
    post: operations["RestartPod"];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  "/endpoints": {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * List endpoints
     * @description Returns a list of endpoints.
     */
    get: operations["ListEndpoints"];
    put?: never;
    /**
     * Create a new endpoint
     * @description Create a new endpoint.
     */
    post: operations["CreateEndpoint"];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  "/endpoints/{endpointId}": {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * Find an endpoint by ID
     * @description Returns a single endpoint.
     */
    get: operations["GetEndpoint"];
    put?: never;
    post?: never;
    /**
     * Delete an endpoint
     * @description Delete an endpoint.
     */
    delete: operations["DeleteEndpoint"];
    options?: never;
    head?: never;
    /**
     * Update an endpoint
     * @description Update an endpoint.
     */
    patch: operations["UpdateEndpoint"];
    trace?: never;
  };
  "/endpoints/{endpointId}/update": {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    /**
     * Update an endpoint
     * @description Update an endpoint - synonym for PATCH /endpoints/{endpointId}.
     */
    post: operations["UpdateEndpoint"];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  "/templates": {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * List templates
     * @description Returns a list of templates.
     */
    get: operations["ListTemplates"];
    put?: never;
    /**
     * Create a new template
     * @description Create a new template.
     */
    post: operations["CreateTemplate"];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  "/templates/{templateId}": {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * Find a template by ID
     * @description Returns a single template.
     */
    get: operations["GetTemplate"];
    put?: never;
    post?: never;
    /**
     * Delete a template
     * @description Delete a template.
     */
    delete: operations["DeleteTemplate"];
    options?: never;
    head?: never;
    /**
     * Update a template
     * @description Update a template.
     */
    patch: operations["UpdateTemplate"];
    trace?: never;
  };
  "/templates/{templateId}/update": {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    /**
     * Update a template
     * @description Update a template - synonym for PATCH /templates/{templateId}.
     */
    post: operations["UpdateTemplate"];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  "/networkvolumes": {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * List network volumes
     * @description Returns a list of network volumes.
     */
    get: operations["ListNetworkVolumes"];
    put?: never;
    /**
     * Create a new network volume
     * @description Create a new network volume.
     */
    post: operations["CreateNetworkVolume"];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  "/networkvolumes/{networkVolumeId}": {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * Find a network volume by ID
     * @description Returns a single network volume.
     */
    get: operations["GetNetworkVolume"];
    put?: never;
    post?: never;
    /**
     * Delete a network volume
     * @description Delete a network volume.
     */
    delete: operations["DeleteNetworkVolume"];
    options?: never;
    head?: never;
    /**
     * Update a network volume
     * @description Update a network volume.
     */
    patch: operations["UpdateNetworkVolume"];
    trace?: never;
  };
  "/networkvolumes/{networkVolumeId}/update": {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    /**
     * Update a network volume
     * @description Update a network volume - synonym for PATCH /networkvolumes/{networkVolumeId}.
     */
    post: operations["UpdateNetworkVolume"];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  "/containerregistryauth": {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * List container registry auths
     * @description Returns a list of container registry auths.
     */
    get: operations["ListContainerRegistryAuths"];
    put?: never;
    /**
     * Create a new container registry auth
     * @description Create a new container registry auth.
     */
    post: operations["CreateContainerRegistryAuth"];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  "/containerregistryauth/{containerRegistryAuthId}": {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * Find a container registry auth by ID
     * @description Returns a single container registry auth.
     */
    get: operations["GetContainerRegistryAuth"];
    put?: never;
    post?: never;
    /**
     * Delete a container registry auth
     * @description Delete a container registry auth.
     */
    delete: operations["DeleteContainerRegistryAuth"];
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  "/billing/pods": {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * Pod billing history
     * @description Retrieve billing information about your Pods.
     */
    get: operations["PodBilling"];
    put?: never;
    post?: never;
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  "/billing/endpoints": {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * Serverless billing history
     * @description Retrieve billing information about your Serverless endpoints.
     */
    get: operations["EndpointBilling"];
    put?: never;
    post?: never;
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  "/billing/networkvolumes": {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * Network volume billing history
     * @description Retrieve billing information about your network volumes.
     */
    get: operations["NetworkVolumeBilling"];
    put?: never;
    post?: never;
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
}
export type webhooks = Record<string, never>;
export interface components {
  schemas: {
    Pods: components["schemas"]["Pod"][];
    Pod: {
      /**
       * @description The effective cost in Runpod credits per hour of running a Pod, adjusted by active Savings Plans.
       * @example 0.69
       */
      adjustedCostPerHr?: number;
      /**
       * @description Synonym for endpointId (legacy name).
       * @example null
       */
      aiApiId?: string;
      /**
       * @description A unique string identifying the Runpod user who rents a Pod.
       * @example user_2PyTJrLzeuwfZilRZ7JhCQDuSqo
       */
      consumerUserId?: string;
      /**
       * @description The amount of disk space, in gigabytes (GB), to allocate on the container disk for a Pod. The data on the container disk is wiped when the Pod restarts. To persist data across Pod restarts, set volumeInGb to configure the Pod network volume.
       * @example 50
       */
      containerDiskInGb?: number;
      /**
       * @description If a Pod is created with a container registry auth, the unique string identifying that container registry auth.
       * @example clzdaifot0001l90809257ynb
       */
      containerRegistryAuthId?: string;
      /**
       * Format: currency
       * @description The cost in Runpod credits per hour of running a Pod. Note that the actual cost may be lower if Savings Plans are applied.
       * @example 0.74
       */
      costPerHr?: number;
      /**
       * @description If the Pod is a CPU Pod, the unique string identifying the CPU flavor the Pod is running on.
       * @example cpu3c
       */
      cpuFlavorId?: string;
      /**
       * @description The current expected status of a Pod.
       * @enum {string}
       */
      desiredStatus?: "RUNNING" | "EXITED" | "TERMINATED";
      /** @description If specified, overrides the ENTRYPOINT for the Docker image run on the created Pod. If [], uses the ENTRYPOINT defined in the image. */
      dockerEntrypoint?: string[];
      /** @description If specified, overrides the start CMD for the Docker image run on the created Pod. If [], uses the start CMD defined in the image. */
      dockerStartCmd?: string[];
      /**
       * @description If the Pod is a Serverless worker, a unique string identifying the associated endpoint.
       * @example null
       */
      endpointId?: string;
      /**
       * @default {}
       * @example {
       *       "ENV_VAR": "value"
       *     }
       */
      env: Record<string, never>;
      gpu?: {
        id?: string;
        /**
         * @description The number of GPUs attached to a Pod.
         * @example 1
         */
        count?: number;
        displayName?: string;
        securePrice?: number;
        communityPrice?: number;
        oneMonthPrice?: number;
        threeMonthPrice?: number;
        sixMonthPrice?: number;
        oneWeekPrice?: number;
        communitySpotPrice?: number;
        secureSpotPrice?: number;
      };
      /**
       * @description A unique string identifying a [Pod](#/components/schema/Pod).
       * @example xedezhzb9la3ye
       */
      id?: string;
      /**
       * @description The image tag for the container run on a Pod.
       * @example runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04
       */
      image?: string;
      /**
       * @description Describes how a Pod is rented. An interruptible Pod can be rented at a lower cost but can be stopped at any time to free up resources for another Pod. A reserved Pod is rented at a higher cost but runs until it exits or is manually stopped.
       * @example false
       */
      interruptible?: boolean;
      /**
       * @description The UTC timestamp when a Pod was last started.
       * @example 2024-07-12T19:14:40.144Z
       */
      lastStartedAt?: string;
      /**
       * @description A string describing the last lifecycle event on a Pod.
       * @example Rented by User: Fri Jul 12 2024 15:14:40 GMT-0400 (Eastern Daylight Time)
       */
      lastStatusChange?: string;
      /**
       * @description Set to true to lock a Pod. Locking a Pod disables stopping or resetting the Pod.
       * @example false
       */
      locked?: boolean;
      /** @description Information about the machine a Pod is running on (see [Machine](#/components/schemas/Machine)). */
      machine?: {
        minPodGpuCount?: number;
        gpuTypeId?: string;
        gpuType?: {
          id?: string;
          /**
           * @description The number of GPUs attached to a Pod.
           * @example 1
           */
          count?: number;
          displayName?: string;
          securePrice?: number;
          communityPrice?: number;
          oneMonthPrice?: number;
          threeMonthPrice?: number;
          sixMonthPrice?: number;
          oneWeekPrice?: number;
          communitySpotPrice?: number;
          secureSpotPrice?: number;
        };
        cpuCount?: number;
        cpuTypeId?: string;
        cpuType?: {
          id?: string;
          displayName?: string;
          cores?: number;
          threadsPerCore?: number;
          groupId?: string;
        };
        location?: string;
        dataCenterId?: string;
        diskThroughputMBps?: number;
        maxDownloadSpeedMbps?: number;
        maxUploadSpeedMbps?: number;
        supportPublicIp?: boolean;
        secureCloud?: boolean;
        maintenanceStart?: string;
        maintenanceEnd?: string;
        maintenanceNote?: string;
        note?: string;
        costPerHr?: number;
        currentPricePerGpu?: number;
        gpuAvailable?: number;
        gpuDisplayName?: string;
      };
      /**
       * @description A unique string identifying the host machine a Pod is running on.
       * @example s194cr8pls2z
       */
      machineId?: string;
      /**
       * @description The amount of RAM, in gigabytes (GB), attached to a Pod.
       * @example 62
       */
      memoryInGb?: number;
      /** @description A user-defined name for the created Pod. The name does not need to be unique. */
      name?: string;
      /** @description If a network volume is attached to a Pod, information about the network volume (see [network volume schema](#/components/schemas/NetworkVolume)). */
      networkVolume?: {
        /**
         * @description A unique string identifying a network volume.
         * @example agv6w2qcg7
         */
        id?: string;
        /**
         * @description A user-defined name for a network volume. The name does not need to be unique.
         * @example my network volume
         */
        name?: string;
        /**
         * @description The amount of disk space, in gigabytes (GB), allocated to a network volume.
         * @example 50
         */
        size?: number;
        /**
         * @description The Runpod data center ID where a network volume is located.
         * @example EU-RO-1
         */
        dataCenterId?: string;
      };
      /**
       * @description A mapping of internal ports to public ports on a Pod. For example, { "22": 10341 } means that port 22 on the Pod is mapped to port 10341 and is publicly accessible at [public ip]:10341. If the Pod is still initializing, this mapping is not yet determined and will be empty.
       * @example {
       *       "22": 10341
       *     }
       */
      portMappings?: Record<string, never> | null;
      /**
       * @description A list of ports exposed on a Pod. Each port is formatted as [port number]/[protocol]. Protocol can be either http or tcp.
       * @example [
       *       "8888/http",
       *       "22/tcp"
       *     ]
       */
      ports?: string[];
      /**
       * Format: ipv4
       * @description The public IP address of a Pod. If the Pod is still initializing, this IP is not yet determined and will be empty.
       * @example 100.65.0.119
       */
      publicIp?: string | null;
      /** @description The list of active Savings Plans applied to a Pod (see [Savings Plans](#/components/schemas/SavingsPlan)). If none are applied, the list is empty. */
      savingsPlans?: components["schemas"]["SavingsPlan"][];
      /**
       * @description If the Pod is a Serverless worker, the version of the associated endpoint (see [Endpoint Version](#/components/schemas/Endpoint/version)).
       * @example 0
       */
      slsVersion?: number;
      /**
       * @description If a Pod is created with a template, the unique string identifying that template.
       * @example null
       */
      templateId?: string;
      /**
       * @description The number of virtual CPUs attached to a Pod.
       * @example 24
       */
      vcpuCount?: number;
      /**
       * @description Set to true if the local network volume of a Pod is encrypted. Can only be set when creating a Pod.
       * @example false
       */
      volumeEncrypted?: boolean;
      /**
       * @description The amount of disk space, in gigabytes (GB), to allocate on the Pod volume for a Pod. The data on the Pod volume is persisted across Pod restarts. To persist data so that future Pods can access it, create a network volume and set networkVolumeId to attach it to the Pod.
       * @example 20
       */
      volumeInGb?: number;
      /**
       * @description If either a Pod volume or a network volume is attached to a Pod, the absolute path where the network volume is mounted in the filesystem.
       * @example /workspace
       */
      volumeMountPath?: string;
    };
    PodUpdateInPlaceInput: {
      /**
       * @description Set to true to lock a Pod. Locking a Pod disables stopping or resetting the Pod.
       * @default false
       */
      locked: boolean;
      /**
       * @description A user-defined name for the created Pod. The name does not need to be unique.
       * @default my pod
       */
      name: string;
    };
    /** @description Input for updating a Pod which will trigger a reset. */
    PodUpdateInput: {
      /**
       * @description The amount of disk space, in gigabytes (GB), to allocate on the container disk for the created Pod. The data on the container disk is wiped when the Pod restarts. To persist data across Pod restarts, set volumeInGb to configure the Pod network volume.
       * @default 50
       */
      containerDiskInGb: number | null;
      /**
       * @description Registry credentials ID.
       * @example clzdaifot0001l90809257ynb
       */
      containerRegistryAuthId?: string;
      /**
       * @description If specified, overrides the ENTRYPOINT for the Docker image run on the created Pod. If [], uses the ENTRYPOINT defined in the image.
       * @default []
       */
      dockerEntrypoint: string[];
      /**
       * @description If specified, overrides the start CMD for the Docker image run on the created Pod. If [], uses the start CMD defined in the image.
       * @default []
       */
      dockerStartCmd: string[];
      /**
       * @default {}
       * @example {
       *       "ENV_VAR": "value"
       *     }
       */
      env: Record<string, never>;
      /**
       * @description Set to true to enable global networking for the created Pod. Currently only available for On-Demand GPU Pods on some Secure Cloud data centers.
       * @default false
       * @example true
       */
      globalNetworking: boolean;
      /**
       * @description The image tag for the container run on the created Pod.
       * @example runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04
       */
      imageName?: string;
      /**
       * @description Set to true to lock a Pod. Locking a Pod disables stopping or resetting the Pod.
       * @default false
       */
      locked: boolean;
      /**
       * @description A user-defined name for the created Pod. The name does not need to be unique.
       * @default my pod
       */
      name: string;
      /**
       * @description A list of ports exposed on the created Pod. Each port is formatted as [port number]/[protocol]. Protocol can be either http or tcp.
       * @default 8888/http,22/tcp
       * @example [
       *       "8888/http",
       *       "22/tcp"
       *     ]
       */
      ports: string[];
      /**
       * @description The amount of disk space, in gigabytes (GB), to allocate on the Pod volume for the created Pod. The data on the Pod volume is persisted across Pod restarts. To persist data so that future Pods can access it, create a network volume and set networkVolumeId to attach it to the Pod.
       * @default 20
       */
      volumeInGb: number | null;
      /**
       * @description If either a Pod volume or a network volume is attached to a Pod, the absolute path where the network volume will be mounted in the filesystem.
       * @default /workspace
       */
      volumeMountPath: string;
    };
    PodCreateInput: {
      /** @description If the created Pod is a GPU Pod, a list of acceptable CUDA versions on the [Pod](#/components/schemas/Pod). If not set, any CUDA version is acceptable. */
      allowedCudaVersions?: (
        | "12.9"
        | "12.8"
        | "12.7"
        | "12.6"
        | "12.5"
        | "12.4"
        | "12.3"
        | "12.2"
        | "12.1"
        | "12.0"
        | "11.8"
      )[];
      /**
       * @description Set to SECURE to create the Pod in Secure Cloud. Set to COMMUNITY to create the Pod in Community Cloud. To determine which one suits your needs, see https://docs.runpod.io/pods/overview#pod-types.
       * @default SECURE
       * @enum {string}
       */
      cloudType: "SECURE" | "COMMUNITY";
      /**
       * @description Set to GPU to create a GPU Pod. Set to CPU to create a CPU Pod. If set to CPU, the Pod will not have a GPU attached and properties related to GPUs such as gpuTypeIds will be ignored. If set to GPU, the Pod will have a GPU attached and properties related to CPUs such as cpuFlavorIds will be ignored.
       * @default GPU
       * @enum {string}
       */
      computeType: "GPU" | "CPU";
      /**
       * @description The amount of disk space, in gigabytes (GB), to allocate on the container disk for the created Pod. The data on the container disk is wiped when the Pod restarts. To persist data across Pod restarts, set volumeInGb to configure the Pod network volume.
       * @default 50
       */
      containerDiskInGb: number | null;
      /**
       * @description Registry credentials ID.
       * @example clzdaifot0001l90809257ynb
       */
      containerRegistryAuthId?: string;
      /** @description A list of country codes where the created Pod can be located. If not set, the Pod can be located in any country. */
      countryCodes?: string[];
      /** @description If the created Pod is a CPU Pod, a list of Runpod CPU flavors which can be attached to the Pod. The order of the list determines the order to rent CPU flavors. See cpuFlavorPriority for how the order of the list affects Pod creation. */
      cpuFlavorIds?: (
        | "cpu3c"
        | "cpu3g"
        | "cpu3m"
        | "cpu5c"
        | "cpu5g"
        | "cpu5m"
      )[];
      /**
       * @description If the created Pod is a CPU Pod, set to availability to respond to current CPU flavor availability. Set to custom to always try to rent CPU flavors in the order specified in cpuFlavorIds.
       * @default availability
       * @enum {string}
       */
      cpuFlavorPriority: "availability" | "custom";
      /**
       * @description A list of Runpod data center IDs where the created Pod can be located. See `dataCenterPriority` for information on how the order of the list affects Pod creation.
       * @default [
       *       "EU-RO-1",
       *       "CA-MTL-1",
       *       "EU-SE-1",
       *       "US-IL-1",
       *       "EUR-IS-1",
       *       "EU-CZ-1",
       *       "US-TX-3",
       *       "EUR-IS-2",
       *       "US-KS-2",
       *       "US-GA-2",
       *       "US-WA-1",
       *       "US-TX-1",
       *       "CA-MTL-3",
       *       "EU-NL-1",
       *       "US-TX-4",
       *       "US-CA-2",
       *       "US-NC-1",
       *       "OC-AU-1",
       *       "US-DE-1",
       *       "EUR-IS-3",
       *       "CA-MTL-2",
       *       "AP-JP-1",
       *       "EUR-NO-1",
       *       "EU-FR-1",
       *       "US-KS-3",
       *       "US-GA-1"
       *     ]
       * @example [
       *       "EU-RO-1",
       *       "CA-MTL-1"
       *     ]
       */
      dataCenterIds: (
        | "EU-RO-1"
        | "CA-MTL-1"
        | "EU-SE-1"
        | "US-IL-1"
        | "EUR-IS-1"
        | "EU-CZ-1"
        | "US-TX-3"
        | "EUR-IS-2"
        | "US-KS-2"
        | "US-GA-2"
        | "US-WA-1"
        | "US-TX-1"
        | "CA-MTL-3"
        | "EU-NL-1"
        | "US-TX-4"
        | "US-CA-2"
        | "US-NC-1"
        | "OC-AU-1"
        | "US-DE-1"
        | "EUR-IS-3"
        | "CA-MTL-2"
        | "AP-JP-1"
        | "EUR-NO-1"
        | "EU-FR-1"
        | "US-KS-3"
        | "US-GA-1"
      )[];
      /**
       * @description Set to availability to respond to current machine availability. Set to custom to always try to rent machines from data centers in the order specified in dataCenterIds.
       * @default availability
       * @enum {string}
       */
      dataCenterPriority: "availability" | "custom";
      /**
       * @description If specified, overrides the ENTRYPOINT for the Docker image run on the created Pod. If [], uses the ENTRYPOINT defined in the image.
       * @default []
       */
      dockerEntrypoint: string[];
      /**
       * @description If specified, overrides the start CMD for the Docker image run on the created Pod. If [], uses the start CMD defined in the image.
       * @default []
       */
      dockerStartCmd: string[];
      /**
       * @default {}
       * @example {
       *       "ENV_VAR": "value"
       *     }
       */
      env: Record<string, never>;
      /**
       * @description Set to true to enable global networking for the created Pod. Currently only available for On-Demand GPU Pods on some Secure Cloud data centers.
       * @default false
       * @example true
       */
      globalNetworking: boolean;
      /**
       * @description If the created Pod is a GPU Pod, the number of GPUs attached to the created Pod.
       * @default 1
       */
      gpuCount: number;
      /** @description If the created Pod is a GPU Pod, a list of Runpod GPU types which can be attached to the created Pod. The order of the list determines the order to rent GPU types. See `gpuTypePriority` for information on how the order of the list affects Pod creation. */
      gpuTypeIds?: (
        | "NVIDIA GeForce RTX 4090"
        | "NVIDIA A40"
        | "NVIDIA RTX A5000"
        | "NVIDIA GeForce RTX 5090"
        | "NVIDIA H100 80GB HBM3"
        | "NVIDIA GeForce RTX 3090"
        | "NVIDIA RTX A4500"
        | "NVIDIA L40S"
        | "NVIDIA H200"
        | "NVIDIA L4"
        | "NVIDIA RTX 6000 Ada Generation"
        | "NVIDIA A100-SXM4-80GB"
        | "NVIDIA RTX 4000 Ada Generation"
        | "NVIDIA RTX A6000"
        | "NVIDIA A100 80GB PCIe"
        | "NVIDIA RTX 2000 Ada Generation"
        | "NVIDIA RTX A4000"
        | "NVIDIA RTX PRO 6000 Blackwell Server Edition"
        | "NVIDIA H100 PCIe"
        | "NVIDIA H100 NVL"
        | "NVIDIA L40"
        | "NVIDIA B200"
        | "NVIDIA GeForce RTX 3080 Ti"
        | "NVIDIA RTX PRO 6000 Blackwell Workstation Edition"
        | "NVIDIA GeForce RTX 3080"
        | "NVIDIA GeForce RTX 3070"
        | "AMD Instinct MI300X OAM"
        | "NVIDIA GeForce RTX 4080 SUPER"
        | "Tesla V100-PCIE-16GB"
        | "Tesla V100-SXM2-32GB"
        | "NVIDIA RTX 5000 Ada Generation"
        | "NVIDIA GeForce RTX 4070 Ti"
        | "NVIDIA RTX 4000 SFF Ada Generation"
        | "NVIDIA GeForce RTX 3090 Ti"
        | "NVIDIA RTX A2000"
        | "NVIDIA GeForce RTX 4080"
        | "NVIDIA A30"
        | "NVIDIA GeForce RTX 5080"
        | "Tesla V100-FHHL-16GB"
        | "NVIDIA H200 NVL"
        | "Tesla V100-SXM2-16GB"
        | "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition"
        | "NVIDIA A5000 Ada"
        | "Tesla V100-PCIE-32GB"
        | "NVIDIA  RTX A4500"
        | "NVIDIA  A30"
        | "NVIDIA GeForce RTX 3080TI"
        | "Tesla T4"
        | "NVIDIA RTX A30"
      )[];
      /**
       * @description If the created Pod is a GPU Pod, set to availability to respond to current GPU type availability. Set to custom to always try to rent GPU types in the order specified in gpuTypeIds.
       * @default availability
       * @enum {string}
       */
      gpuTypePriority: "availability" | "custom";
      /**
       * @description The image tag for the container run on the created Pod.
       * @example runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04
       */
      imageName?: string;
      /**
       * @description Set to true to create an interruptible or spot Pod. An interruptible Pod can be rented at a lower cost but can be stopped at any time to free up resources for another Pod. A reserved Pod is rented at a higher cost but runs until it exits or is manually stopped.
       * @default false
       */
      interruptible: boolean;
      /**
       * @description Set to true to lock a Pod. Locking a Pod disables stopping or resetting the Pod.
       * @default false
       */
      locked: boolean;
      /** @description The minimum disk bandwidth, in megabytes per second (MBps), for the created Pod. */
      minDiskBandwidthMBps?: number;
      /** @description The minimum download speed, in megabits per second (Mbps), for the created Pod. */
      minDownloadMbps?: number;
      /**
       * @description If the created Pod is a GPU Pod, the minimum amount of RAM, in gigabytes (GB), allocated to the created Pod for each GPU attached to the Pod.
       * @default 8
       */
      minRAMPerGPU: number;
      /** @description The minimum upload speed, in megabits per second (Mbps), for the created Pod. */
      minUploadMbps?: number;
      /**
       * @description If the created Pod is a GPU Pod, the minimum number of virtual CPUs allocated to the created Pod for each GPU attached to the Pod.
       * @default 2
       */
      minVCPUPerGPU: number;
      /**
       * @description A user-defined name for the created Pod. The name does not need to be unique.
       * @default my pod
       */
      name: string;
      /** @description The unique string identifying the network volume to attach to the created Pod. If attached, a network volume replaces the Pod network volume. */
      networkVolumeId?: string;
      /**
       * @description A list of ports exposed on the created Pod. Each port is formatted as [port number]/[protocol]. Protocol can be either http or tcp.
       * @default 8888/http,22/tcp
       * @example [
       *       "8888/http",
       *       "22/tcp"
       *     ]
       */
      ports: string[];
      /**
       * @description If the created Pod is on Community Cloud, set to true if you need the Pod to expose a public IP address. If null, the Pod might not have a public IP address. On Secure Cloud, the Pod will always have a public IP address.
       * @example true
       */
      supportPublicIp?: boolean;
      /**
       * @description If the Pod is created with a template, the unique string identifying that template.
       * @example null
       */
      templateId?: string;
      /**
       * @description If the created Pod is a CPU Pod, the number of vCPUs allocated to the Pod.
       * @default 2
       */
      vcpuCount: number;
      /**
       * @description The amount of disk space, in gigabytes (GB), to allocate on the Pod volume for the created Pod. The data on the Pod volume is persisted across Pod restarts. To persist data so that future Pods can access it, create a network volume and set networkVolumeId to attach it to the Pod.
       * @default 20
       */
      volumeInGb: number | null;
      /**
       * @description If either a Pod volume or a network volume is attached to a Pod, the absolute path where the network volume will be mounted in the filesystem.
       * @default /workspace
       */
      volumeMountPath: string;
    };
    NetworkVolumes: {
      /**
       * @description A unique string identifying a network volume.
       * @example agv6w2qcg7
       */
      id?: string;
      /**
       * @description A user-defined name for a network volume. The name does not need to be unique.
       * @example my network volume
       */
      name?: string;
      /**
       * @description The amount of disk space, in gigabytes (GB), allocated to a network volume.
       * @example 50
       */
      size?: number;
      /**
       * @description The Runpod data center ID where a network volume is located.
       * @example EU-RO-1
       */
      dataCenterId?: string;
    }[];
    NetworkVolume: {
      /**
       * @description The Runpod data center ID where a network volume is located.
       * @example EU-RO-1
       */
      dataCenterId?: string;
      /**
       * @description A unique string identifying a network volume.
       * @example agv6w2qcg7
       */
      id?: string;
      /**
       * @description A user-defined name for a network volume. The name does not need to be unique.
       * @example my network volume
       */
      name?: string;
      /**
       * @description The amount of disk space, in gigabytes (GB), allocated to a network volume.
       * @example 50
       */
      size?: number;
    };
    NetworkVolumeCreateInput: {
      /**
       * @description The Runpod data center ID where the created network volume is located.
       * @example EU-RO-1
       */
      dataCenterId: string;
      /**
       * @description A user-defined name for the created network volume. The name does not need to be unique.
       * @example my network volume
       */
      name: string;
      /**
       * @description The amount of disk space, in gigabytes (GB), allocated to the created network volume.
       * @example 50
       */
      size: number;
    };
    NetworkVolumeUpdateInput: {
      /**
       * @description A user-defined name for the network volume. The name does not need to be unique.
       * @example my network volume
       */
      name?: string;
      /**
       * @description The amount of disk space, in gigabytes (GB), which will be allocated to the network volume after the update. Must be greater than the current size of the network volume.
       * @example 50
       */
      size?: number;
    };
    Templates: components["schemas"]["Template"][];
    Template: {
      /**
       * @description The category of the template. The category can be used to filter templates in the Runpod UI. Current categories are NVIDIA, AMD, and CPU.
       * @example NVIDIA
       */
      category?: string;
      /**
       * @description The amount of disk space, in gigabytes (GB), to allocate on the container disk for a Pod or worker. The data on the container disk is wiped when the Pod or worker restarts. To persist data across restarts, set volumeInGb to configure the local network volume.
       * @example 50
       */
      containerDiskInGb?: number;
      containerRegistryAuthId?: string;
      /**
       * @description If specified, overrides the ENTRYPOINT for the Docker image run on a Pod or worker. If [], uses the ENTRYPOINT defined in the image.
       * @example []
       */
      dockerEntrypoint?: string[];
      /**
       * @description If specified, overrides the start CMD for the Docker image run on a Pod or worker. If [], uses the start CMD defined in the image.
       * @example []
       */
      dockerStartCmd?: string[];
      /**
       * @description The amount of Runpod credits earned by the creator of a template by all Pods or workers created from the template.
       * @example 100
       */
      earned?: number;
      /**
       * @default {}
       * @example {
       *       "ENV_VAR": "value"
       *     }
       */
      env: Record<string, never>;
      /**
       * @description A unique string identifying a template.
       * @example 30zmvf89kd
       */
      id?: string;
      /**
       * @description The image tag for the container run on Pods or workers created from a template.
       * @example runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04
       */
      imageName?: string;
      /**
       * @description Set to true if a template is public and can be used by any Runpod user. Set to false if a template is private and can only be used by the creator.
       * @example false
       */
      isPublic?: boolean;
      /**
       * @description If true, a template is an official template managed by Runpod.
       * @example true
       */
      isRunpod?: boolean;
      /**
       * @description If true, instances created from a template are Serverless workers. If false, instances created from a template are Pods.
       * @example true
       */
      isServerless?: boolean;
      /**
       * @description A user-defined name for a template. The name needs to be unique.
       * @example my template
       */
      name?: string;
      /**
       * @description A list of ports exposed on a Pod or worker. Each port is formatted as [port number]/[protocol]. Protocol can be either http or tcp.
       * @example [
       *       "8888/http",
       *       "22/tcp"
       *     ]
       */
      ports?: string[];
      /** @description A string of markdown-formatted text that describes a template. The readme is displayed in the Runpod UI when a user selects the template. */
      readme?: string;
      runtimeInMin?: number;
      /**
       * @description The amount of disk space, in gigabytes (GB), to allocate on the local network volume for a Pod or worker. The data on the local network volume is persisted across restarts. To persist data so that future Pods and workers can access it, create a network volume and set networkVolumeId to attach it to the Pod or worker.
       * @example 20
       */
      volumeInGb?: number;
      /**
       * @description If a local network volume or network volume is attached to a Pod or worker, the absolute path where the network volume is mounted in the filesystem.
       * @example /workspace
       */
      volumeMountPath?: string;
    };
    TemplateCreateInput: {
      /**
       * @description The compute category of the resource defined by this template.
       * @default NVIDIA
       * @enum {string}
       */
      category: "NVIDIA" | "AMD" | "CPU";
      /**
       * @description The amount of disk space in GB to allocate for the container.
       * @default 50
       */
      containerDiskInGb: number;
      /** @description The unique string representing the container auth object needed for a private image. */
      containerRegistryAuthId?: string;
      /**
       * @description If specified, overrides the ENTRYPOINT for the Docker image run on the Pods using this template. If [], uses the ENTRYPOINT defined in the DockerFile.
       * @default []
       */
      dockerEntrypoint: string[];
      /**
       * @description If specified, overrides the start CMD for the Docker image run on the Pods using this template. If [], uses the start CMD defined in the DockerFile.
       * @default []
       */
      dockerStartCmd: string[];
      /**
       * @default {}
       * @example {
       *       "ENV_VAR": "value"
       *     }
       */
      env: Record<string, never>;
      /** @description Docker image name. */
      imageName: string;
      /**
       * @description If this is a Pod template, specifies whether the template is visible to other Runpod users.
       * @default false
       */
      isPublic: boolean;
      /**
       * @description Whether the template specifies a Serverless worker or a Pod.
       * @default false
       */
      isServerless: boolean;
      /** @description Template name. */
      name: string;
      /**
       * @description A list of ports exposed on the created Pod. Each port is formatted as [port number]/[protocol]. Protocol can be either http or tcp.
       * @default 8888/http,22/tcp
       * @example [
       *       "8888/http",
       *       "22/tcp"
       *     ]
       */
      ports: string[];
      /**
       * @description README content in markdown format.
       * @default
       */
      readme: string;
      /**
       * @description The amount of disk space, in gigabytes (GB), to allocate on the Pods deployed with this template.
       * @default 20
       */
      volumeInGb: number;
      /**
       * @description If a volume is attached to a Pod deployed with this template, the absolute path where the volume will be mounted in the filesystem.
       * @default /workspace
       */
      volumeMountPath: string;
    };
    TemplateUpdateInPlaceInput: {
      /**
       * @description If this is a Pod template, specifies whether the template is visible to other Runpod users.
       * @default false
       */
      isPublic: boolean;
      /** @description Template name. */
      name?: string;
      /**
       * @description README content in markdown format.
       * @default
       */
      readme: string;
      /**
       * @description The amount of disk space, in gigabytes (GB), to allocate on the Pods deployed with this template.
       * @default 20
       */
      volumeInGb: number;
      /**
       * @description If a volume is attached to a Pod deployed with this template, the absolute path where the volume will be mounted in the filesystem.
       * @default /workspace
       */
      volumeMountPath: string;
    };
    /** @description Input for updating a Template which will trigger a rolling release for any associated endpoints. */
    TemplateUpdateInput: {
      /**
       * @description The amount of disk space in GB to allocate for the container.
       * @default 50
       */
      containerDiskInGb: number;
      /** @description The unique string representing the container auth object needed for a private image. */
      containerRegistryAuthId?: string;
      /**
       * @description If specified, overrides the ENTRYPOINT for the Docker image run on the Pods using this template. If [], uses the ENTRYPOINT defined in the DockerFile.
       * @default []
       */
      dockerEntrypoint: string[];
      /**
       * @description If specified, overrides the start CMD for the Docker image run on the Pods using this template. If [], uses the start CMD defined in the DockerFile.
       * @default []
       */
      dockerStartCmd: string[];
      /**
       * @default {}
       * @example {
       *       "ENV_VAR": "value"
       *     }
       */
      env: Record<string, never>;
      /** @description Docker image name. */
      imageName?: string;
      /**
       * @description If this is a Pod template, specifies whether the template is visible to other Runpod users.
       * @default false
       */
      isPublic: boolean;
      /** @description Template name. */
      name?: string;
      /**
       * @description A list of ports exposed on the created Pod. Each port is formatted as [port number]/[protocol]. Protocol can be either http or tcp.
       * @default 8888/http,22/tcp
       * @example [
       *       "8888/http",
       *       "22/tcp"
       *     ]
       */
      ports: string[];
      /**
       * @description README content in markdown format.
       * @default
       */
      readme: string;
      /**
       * @description The amount of disk space, in gigabytes (GB), to allocate on the Pods deployed with this template.
       * @default 20
       */
      volumeInGb: number;
      /**
       * @description If a volume is attached to a Pod deployed with this template, the absolute path where the volume will be mounted in the filesystem.
       * @default /workspace
       */
      volumeMountPath: string;
    };
    Endpoints: components["schemas"]["Endpoint"][];
    Endpoint: {
      /** @description A list of acceptable CUDA versions for the workers on a Serverless endpoint. If not set, any CUDA version is acceptable. */
      allowedCudaVersions?: (
        | "12.9"
        | "12.8"
        | "12.7"
        | "12.6"
        | "12.5"
        | "12.4"
        | "12.3"
        | "12.2"
        | "12.1"
        | "12.0"
        | "11.8"
      )[];
      /**
       * @description The type of compute used by workers on a Serverless endpoint.
       * @example GPU
       * @enum {string}
       */
      computeType?: "CPU" | "GPU";
      /**
       * @description The UTC timestamp when a Serverless endpoint was created.
       * @example 2024-07-12T19:14:40.144Z
       */
      createdAt?: string;
      /**
       * @description A list of Runpod data center IDs where workers on a Serverless endpoint can be located.
       * @default [
       *       "EU-RO-1",
       *       "CA-MTL-1",
       *       "EU-SE-1",
       *       "US-IL-1",
       *       "EUR-IS-1",
       *       "EU-CZ-1",
       *       "US-TX-3",
       *       "EUR-IS-2",
       *       "US-KS-2",
       *       "US-GA-2",
       *       "US-WA-1",
       *       "US-TX-1",
       *       "CA-MTL-3",
       *       "EU-NL-1",
       *       "US-TX-4",
       *       "US-CA-2",
       *       "US-NC-1",
       *       "OC-AU-1",
       *       "US-DE-1",
       *       "EUR-IS-3",
       *       "CA-MTL-2",
       *       "AP-JP-1",
       *       "EUR-NO-1",
       *       "EU-FR-1",
       *       "US-KS-3",
       *       "US-GA-1"
       *     ]
       * @example EU-NL-1,EU-RO-1,EU-SE-1
       */
      dataCenterIds: (
        | "EU-RO-1"
        | "CA-MTL-1"
        | "EU-SE-1"
        | "US-IL-1"
        | "EUR-IS-1"
        | "EU-CZ-1"
        | "US-TX-3"
        | "EUR-IS-2"
        | "US-KS-2"
        | "US-GA-2"
        | "US-WA-1"
        | "US-TX-1"
        | "CA-MTL-3"
        | "EU-NL-1"
        | "US-TX-4"
        | "US-CA-2"
        | "US-NC-1"
        | "OC-AU-1"
        | "US-DE-1"
        | "EUR-IS-3"
        | "CA-MTL-2"
        | "AP-JP-1"
        | "EUR-NO-1"
        | "EU-FR-1"
        | "US-KS-3"
        | "US-GA-1"
      )[];
      /**
       * @default {}
       * @example {
       *       "ENV_VAR": "value"
       *     }
       */
      env: Record<string, never>;
      /**
       * @description The maximum number of milliseconds an individual request can run on a Serverless endpoint before the worker is stopped and the request is marked as failed.
       * @example 600000
       */
      executionTimeoutMs?: number;
      /**
       * @description The number of GPUs attached to each worker on a Serverless endpoint.
       * @example 1
       */
      gpuCount?: number;
      /** @description A list of Runpod GPU types which can be attached to a Serverless endpoint. */
      gpuTypeIds?: (
        | "NVIDIA GeForce RTX 4090"
        | "NVIDIA A40"
        | "NVIDIA RTX A5000"
        | "NVIDIA GeForce RTX 5090"
        | "NVIDIA H100 80GB HBM3"
        | "NVIDIA GeForce RTX 3090"
        | "NVIDIA RTX A4500"
        | "NVIDIA L40S"
        | "NVIDIA H200"
        | "NVIDIA L4"
        | "NVIDIA RTX 6000 Ada Generation"
        | "NVIDIA A100-SXM4-80GB"
        | "NVIDIA RTX 4000 Ada Generation"
        | "NVIDIA RTX A6000"
        | "NVIDIA A100 80GB PCIe"
        | "NVIDIA RTX 2000 Ada Generation"
        | "NVIDIA RTX A4000"
        | "NVIDIA RTX PRO 6000 Blackwell Server Edition"
        | "NVIDIA H100 PCIe"
        | "NVIDIA H100 NVL"
        | "NVIDIA L40"
        | "NVIDIA B200"
        | "NVIDIA GeForce RTX 3080 Ti"
        | "NVIDIA RTX PRO 6000 Blackwell Workstation Edition"
        | "NVIDIA GeForce RTX 3080"
        | "NVIDIA GeForce RTX 3070"
        | "AMD Instinct MI300X OAM"
        | "NVIDIA GeForce RTX 4080 SUPER"
        | "Tesla V100-PCIE-16GB"
        | "Tesla V100-SXM2-32GB"
        | "NVIDIA RTX 5000 Ada Generation"
        | "NVIDIA GeForce RTX 4070 Ti"
        | "NVIDIA RTX 4000 SFF Ada Generation"
        | "NVIDIA GeForce RTX 3090 Ti"
        | "NVIDIA RTX A2000"
        | "NVIDIA GeForce RTX 4080"
        | "NVIDIA A30"
        | "NVIDIA GeForce RTX 5080"
        | "Tesla V100-FHHL-16GB"
        | "NVIDIA H200 NVL"
        | "Tesla V100-SXM2-16GB"
        | "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition"
        | "NVIDIA A5000 Ada"
        | "Tesla V100-PCIE-32GB"
        | "NVIDIA  RTX A4500"
        | "NVIDIA  A30"
        | "NVIDIA GeForce RTX 3080TI"
        | "Tesla T4"
        | "NVIDIA RTX A30"
      )[];
      /**
       * @description A unique string identifying a Serverless endpoint.
       * @example jpnw0v75y3qoql
       */
      id?: string;
      /**
       * @description The number of seconds a worker on a Serverless endpoint can be running without taking a job before the worker is scaled down.
       * @example 5
       */
      idleTimeout?: number;
      /**
       * @description For CPU Serverless endpoints, a list of instance IDs that can be attached to a Serverless endpoint.
       * @example [
       *       "cpu3c-8-16"
       *     ]
       */
      instanceIds?: string[];
      /**
       * @description A user-defined name for a Serverless endpoint. The name does not need to be unique.
       * @example my endpoint
       */
      name?: string;
      /**
       * @description The unique string identifying the network volume to attach to the Serverless endpoint.
       * @example agv6w2qcg7
       */
      networkVolumeId?: string;
      /**
       * @description A list of network volume IDs attached to the Serverless endpoint. Allows multiple network volumes to be used with multi-region endpoints.
       * @example [
       *       "agv6w2qcg7",
       *       "bxh7w3rch8"
       *     ]
       */
      networkVolumeIds?: string[];
      /**
       * @description The method used to scale up workers on a Serverless endpoint. If QUEUE_DELAY, workers are scaled based on a periodic check to see if any requests have been in queue for too long. If REQUEST_COUNT, the desired number of workers is periodically calculated based on the number of requests in the endpoint's queue. Use QUEUE_DELAY if you need to ensure requests take no longer than a maximum latency, and use REQUEST_COUNT if you need to scale based on the number of requests.
       * @example QUEUE_DELAY
       * @enum {string}
       */
      scalerType?: "QUEUE_DELAY" | "REQUEST_COUNT";
      /**
       * @description If the endpoint scalerType is QUEUE_DELAY, the number of seconds a request can remain in queue before a new worker is scaled up. If the endpoint scalerType is REQUEST_COUNT, the number of workers is increased as needed to meet the number of requests in the endpoint's queue divided by scalerValue.
       * @example 4
       */
      scalerValue?: number;
      template?: components["schemas"]["Template"];
      /**
       * @description The unique string identifying the template used to create a Serverless endpoint.
       * @example 30zmvf89kd
       */
      templateId?: string;
      /**
       * @description A unique string identifying the Runpod user who created a Serverless endpoint.
       * @example user_2PyTJrLzeuwfZilRZ7JhCQDuSqo
       */
      userId?: string;
      /**
       * @description The latest version of a Serverless endpoint, which is updated whenever the template or environment variables of the endpoint are changed.
       * @example 0
       */
      version?: number;
      /** @description Information about current workers on a Serverless endpoint. */
      workers?: components["schemas"]["Pod"][];
      /**
       * @description The maximum number of workers that can be running at the same time on a Serverless endpoint.
       * @example 3
       */
      workersMax?: number;
      /**
       * @description The minimum number of workers that will run at the same time on a Serverless endpoint. This number of workers will always stay running for the endpoint, and will be charged even if no requests are being processed, but they are charged at a lower rate than running autoscaling workers.
       * @example 0
       */
      workersMin?: number;
    };
    EndpointUpdateInPlaceInput: {
      /**
       * @description The maximum number of milliseconds an individual request can run on a Serverless endpoint before the worker is stopped and the request is marked as failed.
       * @example 600000
       */
      executionTimeoutMs?: number;
      /**
       * @description Whether to use flash boot for the created Serverless endpoint.
       * @example true
       */
      flashboot?: boolean;
      /**
       * @description The number of seconds a worker on the created Serverless endpoint can run without taking a job before the worker is scaled down.
       * @default 5
       */
      idleTimeout: number;
      /**
       * @description A user-defined name for a Serverless endpoint. The name does not need to be unique.
       * @example my endpoint
       */
      name?: string;
      /**
       * @description The method used to scale up workers on the created Serverless endpoint. If QUEUE_DELAY, workers are scaled based on a periodic check to see if any requests have been in queue for too long. If REQUEST_COUNT, the desired number of workers is periodically calculated based on the number of requests in the endpoint's queue. Use QUEUE_DELAY if you need to ensure requests take no longer than a maximum latency, and use REQUEST_COUNT if you need to scale based on the number of requests.
       * @default QUEUE_DELAY
       * @enum {string}
       */
      scalerType: "QUEUE_DELAY" | "REQUEST_COUNT";
      /**
       * @description If the endpoint scalerType is QUEUE_DELAY, the number of seconds a request can remain in queue before a new worker is scaled up. If the endpoint scalerType is REQUEST_COUNT, the number of workers is increased as needed to meet the number of requests in the endpoint's queue divided by scalerValue.
       * @default 4
       */
      scalerValue: number;
      /**
       * @description The maximum number of workers that can be running at the same time on a Serverless endpoint.
       * @example 3
       */
      workersMax?: number;
      /**
       * @description The minimum number of workers that will run at the same time on a Serverless endpoint. This number of workers will always stay running for the endpoint, and will be charged even if no requests are being processed, but they are charged at a lower rate than running autoscaling workers.
       * @example 0
       */
      workersMin?: number;
    };
    /** @description Input for updating an endpoint which will trigger a rolling release on the endpoint. */
    EndpointUpdateInput: {
      /** @description If the created Serverless endpoint is a GPU endpoint, a list of acceptable CUDA versions on the created workers. If not set, any CUDA version is acceptable. */
      allowedCudaVersions?: (
        | "12.9"
        | "12.8"
        | "12.7"
        | "12.6"
        | "12.5"
        | "12.4"
        | "12.3"
        | "12.2"
        | "12.1"
        | "12.0"
        | "11.8"
      )[];
      /** @description If the created Serverless endpoint is a CPU endpoint, a list of Runpod CPU flavors which can be attached to the created workers. The order of the list determines the order to rent CPU flavors. */
      cpuFlavorIds?: ("cpu3c" | "cpu3g" | "cpu5c" | "cpu5g")[];
      /**
       * @description A list of Runpod data center IDs where workers on the created Serverless endpoint can be located.
       * @default [
       *       "EU-RO-1",
       *       "CA-MTL-1",
       *       "EU-SE-1",
       *       "US-IL-1",
       *       "EUR-IS-1",
       *       "EU-CZ-1",
       *       "US-TX-3",
       *       "EUR-IS-2",
       *       "US-KS-2",
       *       "US-GA-2",
       *       "US-WA-1",
       *       "US-TX-1",
       *       "CA-MTL-3",
       *       "EU-NL-1",
       *       "US-TX-4",
       *       "US-CA-2",
       *       "US-NC-1",
       *       "OC-AU-1",
       *       "US-DE-1",
       *       "EUR-IS-3",
       *       "CA-MTL-2",
       *       "AP-JP-1",
       *       "EUR-NO-1",
       *       "EU-FR-1",
       *       "US-KS-3",
       *       "US-GA-1"
       *     ]
       * @example [
       *       "EU-RO-1",
       *       "CA-MTL-1"
       *     ]
       */
      dataCenterIds: (
        | "EU-RO-1"
        | "CA-MTL-1"
        | "EU-SE-1"
        | "US-IL-1"
        | "EUR-IS-1"
        | "EU-CZ-1"
        | "US-TX-3"
        | "EUR-IS-2"
        | "US-KS-2"
        | "US-GA-2"
        | "US-WA-1"
        | "US-TX-1"
        | "CA-MTL-3"
        | "EU-NL-1"
        | "US-TX-4"
        | "US-CA-2"
        | "US-NC-1"
        | "OC-AU-1"
        | "US-DE-1"
        | "EUR-IS-3"
        | "CA-MTL-2"
        | "AP-JP-1"
        | "EUR-NO-1"
        | "EU-FR-1"
        | "US-KS-3"
        | "US-GA-1"
      )[];
      /**
       * @description The maximum number of milliseconds an individual request can run on a Serverless endpoint before the worker is stopped and the request is marked as failed.
       * @example 600000
       */
      executionTimeoutMs?: number;
      /**
       * @description Whether to use flash boot for the created Serverless endpoint.
       * @example true
       */
      flashboot?: boolean;
      /**
       * @description If the created Serverless endpoint is a GPU endpoint, the number of GPUs attached to each worker on the endpoint.
       * @default 1
       */
      gpuCount: number;
      /** @description If the created Serverless endpoint is a GPU endpoint, a list of Runpod GPU types which can be attached to the created workers. The order of the list determines the order to rent GPU types. */
      gpuTypeIds?: (
        | "NVIDIA GeForce RTX 4090"
        | "NVIDIA A40"
        | "NVIDIA RTX A5000"
        | "NVIDIA GeForce RTX 5090"
        | "NVIDIA H100 80GB HBM3"
        | "NVIDIA GeForce RTX 3090"
        | "NVIDIA RTX A4500"
        | "NVIDIA L40S"
        | "NVIDIA H200"
        | "NVIDIA L4"
        | "NVIDIA RTX 6000 Ada Generation"
        | "NVIDIA A100-SXM4-80GB"
        | "NVIDIA RTX 4000 Ada Generation"
        | "NVIDIA RTX A6000"
        | "NVIDIA A100 80GB PCIe"
        | "NVIDIA RTX 2000 Ada Generation"
        | "NVIDIA RTX A4000"
        | "NVIDIA RTX PRO 6000 Blackwell Server Edition"
        | "NVIDIA H100 PCIe"
        | "NVIDIA H100 NVL"
        | "NVIDIA L40"
        | "NVIDIA B200"
        | "NVIDIA GeForce RTX 3080 Ti"
        | "NVIDIA RTX PRO 6000 Blackwell Workstation Edition"
        | "NVIDIA GeForce RTX 3080"
        | "NVIDIA GeForce RTX 3070"
        | "AMD Instinct MI300X OAM"
        | "NVIDIA GeForce RTX 4080 SUPER"
        | "Tesla V100-PCIE-16GB"
        | "Tesla V100-SXM2-32GB"
        | "NVIDIA RTX 5000 Ada Generation"
        | "NVIDIA GeForce RTX 4070 Ti"
        | "NVIDIA RTX 4000 SFF Ada Generation"
        | "NVIDIA GeForce RTX 3090 Ti"
        | "NVIDIA RTX A2000"
        | "NVIDIA GeForce RTX 4080"
        | "NVIDIA A30"
        | "NVIDIA GeForce RTX 5080"
        | "Tesla V100-FHHL-16GB"
        | "NVIDIA H200 NVL"
        | "Tesla V100-SXM2-16GB"
        | "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition"
        | "NVIDIA A5000 Ada"
        | "Tesla V100-PCIE-32GB"
        | "NVIDIA  RTX A4500"
        | "NVIDIA  A30"
        | "NVIDIA GeForce RTX 3080TI"
        | "Tesla T4"
        | "NVIDIA RTX A30"
      )[];
      /**
       * @description The number of seconds a worker on the created Serverless endpoint can run without taking a job before the worker is scaled down.
       * @default 5
       */
      idleTimeout: number;
      /** @description A user-defined name for the created Serverless endpoint. The name does not need to be unique. */
      name?: string;
      /** @description The unique string identifying the network volume to attach to the created Serverless endpoint. */
      networkVolumeId?: string;
      /** @description A list of network volume IDs to attach to the created Serverless endpoint. Allows multiple network volumes to be used with multi-region endpoints. */
      networkVolumeIds?: string[];
      /**
       * @description The method used to scale up workers on the created Serverless endpoint. If QUEUE_DELAY, workers are scaled based on a periodic check to see if any requests have been in queue for too long. If REQUEST_COUNT, the desired number of workers is periodically calculated based on the number of requests in the endpoint's queue. Use QUEUE_DELAY if you need to ensure requests take no longer than a maximum latency, and use REQUEST_COUNT if you need to scale based on the number of requests.
       * @default QUEUE_DELAY
       * @enum {string}
       */
      scalerType: "QUEUE_DELAY" | "REQUEST_COUNT";
      /**
       * @description If the endpoint scalerType is QUEUE_DELAY, the number of seconds a request can remain in queue before a new worker is scaled up. If the endpoint scalerType is REQUEST_COUNT, the number of workers is increased as needed to meet the number of requests in the endpoint's queue divided by scalerValue.
       * @default 4
       */
      scalerValue: number;
      /**
       * @description The unique string identifying the template used to create the Serverless endpoint.
       * @example 30zmvf89kd
       */
      templateId?: string;
      /**
       * @description If the created Serverless endpoint is a CPU endpoint, the number of vCPUs allocated to each created worker.
       * @default 2
       */
      vcpuCount: number;
      /**
       * @description The maximum number of workers that can be running at the same time on a Serverless endpoint.
       * @example 3
       */
      workersMax?: number;
      /**
       * @description The minimum number of workers that will run at the same time on a Serverless endpoint. This number of workers will always stay running for the endpoint, and will be charged even if no requests are being processed, but they are charged at a lower rate than running autoscaling workers.
       * @example 0
       */
      workersMin?: number;
    };
    EndpointCreateInput: {
      /** @description If the created Serverless endpoint is a GPU endpoint, a list of acceptable CUDA versions on the created workers. If not set, any CUDA version is acceptable. */
      allowedCudaVersions?: (
        | "12.9"
        | "12.8"
        | "12.7"
        | "12.6"
        | "12.5"
        | "12.4"
        | "12.3"
        | "12.2"
        | "12.1"
        | "12.0"
        | "11.8"
      )[];
      /**
       * @description Set to GPU to create a Serverless endpoint with GPU workers. Set to CPU to create a Serverless endpoint with CPU workers. If set to CPU, properties related to GPUs such as gpuTypeIds will be ignored. If set to GPU, properties related to CPUs such as cpuFlavorIds will be ignored.
       * @default GPU
       * @enum {string}
       */
      computeType: "GPU" | "CPU";
      /** @description If the created Serverless endpoint is a CPU endpoint, a list of Runpod CPU flavors which can be attached to the created workers. The order of the list determines the order to rent CPU flavors. */
      cpuFlavorIds?: ("cpu3c" | "cpu3g" | "cpu5c" | "cpu5g")[];
      /**
       * @description A list of Runpod data center IDs where workers on the created Serverless endpoint can be located.
       * @default [
       *       "EU-RO-1",
       *       "CA-MTL-1",
       *       "EU-SE-1",
       *       "US-IL-1",
       *       "EUR-IS-1",
       *       "EU-CZ-1",
       *       "US-TX-3",
       *       "EUR-IS-2",
       *       "US-KS-2",
       *       "US-GA-2",
       *       "US-WA-1",
       *       "US-TX-1",
       *       "CA-MTL-3",
       *       "EU-NL-1",
       *       "US-TX-4",
       *       "US-CA-2",
       *       "US-NC-1",
       *       "OC-AU-1",
       *       "US-DE-1",
       *       "EUR-IS-3",
       *       "CA-MTL-2",
       *       "AP-JP-1",
       *       "EUR-NO-1",
       *       "EU-FR-1",
       *       "US-KS-3",
       *       "US-GA-1"
       *     ]
       * @example [
       *       "EU-RO-1",
       *       "CA-MTL-1"
       *     ]
       */
      dataCenterIds: (
        | "EU-RO-1"
        | "CA-MTL-1"
        | "EU-SE-1"
        | "US-IL-1"
        | "EUR-IS-1"
        | "EU-CZ-1"
        | "US-TX-3"
        | "EUR-IS-2"
        | "US-KS-2"
        | "US-GA-2"
        | "US-WA-1"
        | "US-TX-1"
        | "CA-MTL-3"
        | "EU-NL-1"
        | "US-TX-4"
        | "US-CA-2"
        | "US-NC-1"
        | "OC-AU-1"
        | "US-DE-1"
        | "EUR-IS-3"
        | "CA-MTL-2"
        | "AP-JP-1"
        | "EUR-NO-1"
        | "EU-FR-1"
        | "US-KS-3"
        | "US-GA-1"
      )[];
      /**
       * @description The maximum number of milliseconds an individual request can run on a Serverless endpoint before the worker is stopped and the request is marked as failed.
       * @example 600000
       */
      executionTimeoutMs?: number;
      /**
       * @description Whether to use flash boot for the created Serverless endpoint.
       * @example true
       */
      flashboot?: boolean;
      /**
       * @description If the created Serverless endpoint is a GPU endpoint, the number of GPUs attached to each worker on the endpoint.
       * @default 1
       */
      gpuCount: number;
      /** @description If the created Serverless endpoint is a GPU endpoint, a list of Runpod GPU types which can be attached to the created workers. The order of the list determines the order to rent GPU types. */
      gpuTypeIds?: (
        | "NVIDIA GeForce RTX 4090"
        | "NVIDIA A40"
        | "NVIDIA RTX A5000"
        | "NVIDIA GeForce RTX 5090"
        | "NVIDIA H100 80GB HBM3"
        | "NVIDIA GeForce RTX 3090"
        | "NVIDIA RTX A4500"
        | "NVIDIA L40S"
        | "NVIDIA H200"
        | "NVIDIA L4"
        | "NVIDIA RTX 6000 Ada Generation"
        | "NVIDIA A100-SXM4-80GB"
        | "NVIDIA RTX 4000 Ada Generation"
        | "NVIDIA RTX A6000"
        | "NVIDIA A100 80GB PCIe"
        | "NVIDIA RTX 2000 Ada Generation"
        | "NVIDIA RTX A4000"
        | "NVIDIA RTX PRO 6000 Blackwell Server Edition"
        | "NVIDIA H100 PCIe"
        | "NVIDIA H100 NVL"
        | "NVIDIA L40"
        | "NVIDIA B200"
        | "NVIDIA GeForce RTX 3080 Ti"
        | "NVIDIA RTX PRO 6000 Blackwell Workstation Edition"
        | "NVIDIA GeForce RTX 3080"
        | "NVIDIA GeForce RTX 3070"
        | "AMD Instinct MI300X OAM"
        | "NVIDIA GeForce RTX 4080 SUPER"
        | "Tesla V100-PCIE-16GB"
        | "Tesla V100-SXM2-32GB"
        | "NVIDIA RTX 5000 Ada Generation"
        | "NVIDIA GeForce RTX 4070 Ti"
        | "NVIDIA RTX 4000 SFF Ada Generation"
        | "NVIDIA GeForce RTX 3090 Ti"
        | "NVIDIA RTX A2000"
        | "NVIDIA GeForce RTX 4080"
        | "NVIDIA A30"
        | "NVIDIA GeForce RTX 5080"
        | "Tesla V100-FHHL-16GB"
        | "NVIDIA H200 NVL"
        | "Tesla V100-SXM2-16GB"
        | "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition"
        | "NVIDIA A5000 Ada"
        | "Tesla V100-PCIE-32GB"
        | "NVIDIA  RTX A4500"
        | "NVIDIA  A30"
        | "NVIDIA GeForce RTX 3080TI"
        | "Tesla T4"
        | "NVIDIA RTX A30"
      )[];
      /**
       * @description The number of seconds a worker on the created Serverless endpoint can run without taking a job before the worker is scaled down.
       * @default 5
       */
      idleTimeout: number;
      /** @description A user-defined name for the created Serverless endpoint. The name does not need to be unique. */
      name?: string;
      /** @description The unique string identifying the network volume to attach to the created Serverless endpoint. */
      networkVolumeId?: string;
      /** @description A list of network volume IDs to attach to the created Serverless endpoint. Allows multiple network volumes to be used with multi-region endpoints. */
      networkVolumeIds?: string[];
      /**
       * @description The method used to scale up workers on the created Serverless endpoint. If QUEUE_DELAY, workers are scaled based on a periodic check to see if any requests have been in queue for too long. If REQUEST_COUNT, the desired number of workers is periodically calculated based on the number of requests in the endpoint's queue. Use QUEUE_DELAY if you need to ensure requests take no longer than a maximum latency, and use REQUEST_COUNT if you need to scale based on the number of requests.
       * @default QUEUE_DELAY
       * @enum {string}
       */
      scalerType: "QUEUE_DELAY" | "REQUEST_COUNT";
      /**
       * @description If the endpoint scalerType is QUEUE_DELAY, the number of seconds a request can remain in queue before a new worker is scaled up. If the endpoint scalerType is REQUEST_COUNT, the number of workers is increased as needed to meet the number of requests in the endpoint's queue divided by scalerValue.
       * @default 4
       */
      scalerValue: number;
      /**
       * @description The unique string identifying the template used to create the Serverless endpoint.
       * @example 30zmvf89kd
       */
      templateId: string;
      /**
       * @description If the created Serverless endpoint is a CPU endpoint, the number of vCPUs allocated to each created worker.
       * @default 2
       */
      vcpuCount: number;
      /**
       * @description The maximum number of workers that can be running at the same time on a Serverless endpoint.
       * @example 3
       */
      workersMax?: number;
      /**
       * @description The minimum number of workers that will run at the same time on a Serverless endpoint. This number of workers will always stay running for the endpoint, and will be charged even if no requests are being processed, but they are charged at a lower rate than running autoscaling workers.
       * @example 0
       */
      workersMin?: number;
    };
    User: string;
    SavingsPlan: {
      /** @example 0.21 */
      costPerHr?: number;
      /** @example 2024-07-12T19:14:40.144Z */
      endTime?: string;
      /** @example NVIDIA GeForce RTX 4090 */
      gpuTypeId?: string;
      /** @example clkrb4qci0000mb09c7sualzo */
      id?: string;
      /** @example xedezhzb9la3ye */
      podId?: string;
      /** @example 2024-05-12T19:14:40.144Z */
      startTime?: string;
    };
    Machine: {
      costPerHr?: number;
      cpuCount?: number;
      cpuType?: {
        id?: string;
        displayName?: string;
        cores?: number;
        threadsPerCore?: number;
        groupId?: string;
      };
      cpuTypeId?: string;
      currentPricePerGpu?: number;
      dataCenterId?: string;
      diskThroughputMBps?: number;
      gpuAvailable?: number;
      gpuDisplayName?: string;
      gpuType?: {
        id?: string;
        /**
         * @description The number of GPUs attached to a Pod.
         * @example 1
         */
        count?: number;
        displayName?: string;
        securePrice?: number;
        communityPrice?: number;
        oneMonthPrice?: number;
        threeMonthPrice?: number;
        sixMonthPrice?: number;
        oneWeekPrice?: number;
        communitySpotPrice?: number;
        secureSpotPrice?: number;
      };
      gpuTypeId?: string;
      location?: string;
      maintenanceEnd?: string;
      maintenanceNote?: string;
      maintenanceStart?: string;
      maxDownloadSpeedMbps?: number;
      maxUploadSpeedMbps?: number;
      minPodGpuCount?: number;
      note?: string;
      secureCloud?: boolean;
      supportPublicIp?: boolean;
    };
    DataCenter: {
      id?: string;
    };
    UnauthorizedError: {
      message?: string;
    };
    /** @enum {string} */
    CudaVersions: "12.4" | "12.3" | "12.2" | "12.1" | "12.0" | "11.8";
    /** @enum {string} */
    GPUTypeId:
      | "NVIDIA GeForce RTX 4090"
      | "NVIDIA RTX A5000"
      | "NVIDIA RTX A4000"
      | "NVIDIA GeForce RTX 3090"
      | "NVIDIA RTX A6000"
      | "NVIDIA A40"
      | "NVIDIA RTX A4500"
      | "NVIDIA A100 80GB PCIe"
      | "NVIDIA L4"
      | "NVIDIA RTX 4000 Ada Generation"
      | "NVIDIA RTX 6000 Ada Generation"
      | "NVIDIA A100-SXM4-80GB"
      | "NVIDIA H100 80GB HBM3"
      | "NVIDIA L40"
      | "NVIDIA H100 PCIe"
      | "NVIDIA L40S"
      | "NVIDIA GeForce RTX 3080"
      | "NVIDIA GeForce RTX 3070"
      | "NVIDIA GeForce RTX 3080 Ti"
      | "NVIDIA A30"
      | "NVIDIA GeForce RTX 4080"
      | "NVIDIA RTX A2000"
      | "NVIDIA GeForce RTX 3090 Ti"
      | "Tesla V100-SXM2-32GB"
      | "NVIDIA GeForce RTX 4070 Ti"
      | "NVIDIA RTX 4000 SFF Ada Generation"
      | "NVIDIA RTX 5000 Ada Generation"
      | "Tesla V100-SXM2-16GB"
      | "Tesla V100-FHHL-16GB"
      | "Tesla V100-PCIE-16GB"
      | "NVIDIA RTX 2000 Ada Generation"
      | "NVIDIA H100 NVL"
      | "AMD Instinct MI300X OAM"
      | "NVIDIA A100-SXM4-40GB";
    ContainerRegistryAuth: {
      /**
       * @description A unique string identifying a container registry authentication.
       * @example clzdaifot0001l90809257ynb
       */
      id?: string;
      /**
       * @description A user-defined name for a container registry authentication. The name must be unique.
       * @example my creds
       */
      name?: string;
    };
    ContainerRegistryAuths: components["schemas"]["ContainerRegistryAuth"][];
    ContainerRegistryAuthCreateInput: {
      /**
       * @description A user-defined name for a container registry authentication. The name must be unique.
       * @example my creds
       */
      name: string;
      /**
       * @description The password for the container registry.
       * @example my-password
       */
      password: string;
      /**
       * @description The username for the container registry.
       * @example my-username
       */
      username: string;
    };
    BillingRecord: {
      /**
       * @description The amount charged for the group for the billing period, in USD.
       * @example 100.5
       */
      amount?: number;
      /**
       * @description The amount of disk space billed for the billing period, in gigabytes (GB). Does not apply to all resource types.
       * @example 50
       */
      diskSpaceBilledGb?: number;
      /** @description If grouping by endpoint ID, the endpoint ID of the group. */
      endpointId?: string;
      /** @description If grouping by GPU type ID, the GPU type ID of the group. */
      gpuTypeId?: string;
      /** @description If grouping by Pod ID, the Pod ID of the group. */
      podId?: string;
      /**
       * Format: date-time
       * @description The start of the period for which the billing record applies.
       * @example 2023-01-01T00:00:00Z
       */
      time?: string;
      /**
       * @description The total time billed for the billing period, in milliseconds. Does not apply to all resource types.
       * @example 3600000
       */
      timeBilledMs?: number;
    };
    BillingRecords: {
      /**
       * @description The amount charged for the group for the billing period, in USD.
       * @example 100.5
       */
      amount?: number;
      /**
       * @description The amount of disk space billed for the billing period, in gigabytes (GB). Does not apply to all resource types.
       * @example 50
       */
      diskSpaceBilledGb?: number;
      /** @description If grouping by endpoint ID, the endpoint ID of the group. */
      endpointId?: string;
      /** @description If grouping by GPU type ID, the GPU type ID of the group. */
      gpuTypeId?: string;
      /** @description If grouping by Pod ID, the Pod ID of the group. */
      podId?: string;
      /**
       * Format: date-time
       * @description The start of the period for which the billing record applies.
       * @example 2023-01-01T00:00:00Z
       */
      time?: string;
      /**
       * @description The total time billed for the billing period, in milliseconds. Does not apply to all resource types.
       * @example 3600000
       */
      timeBilledMs?: number;
    }[];
    NetworkVolumeBillingRecord: {
      /**
       * @description The amount charged for the group for the billing period, in USD.
       * @example 100.5
       */
      amount?: number;
      /**
       * @description The amount of disk space billed for the billing period, in gigabytes (GB). Does not apply to all resource types.
       * @example 50
       */
      diskSpaceBilledGb?: number;
      /**
       * @description The amount charged for high performance storage for the billing period, in USD.
       * @example 100.5
       */
      highPerformanceStorageAmount?: number;
      /**
       * @description The amount of high performance storage disk space billed for the billing period, in gigabytes (GB).
       * @example 50
       */
      highPerformanceStorageDiskSpaceBilledGb?: number;
      /**
       * Format: date-time
       * @description The start of the period for which the billing record applies.
       * @example 2023-01-01T00:00:00Z
       */
      time?: string;
    };
    NetworkVolumeBillingRecords: {
      /**
       * @description The amount charged for the group for the billing period, in USD.
       * @example 100.5
       */
      amount?: number;
      /**
       * @description The amount of disk space billed for the billing period, in gigabytes (GB). Does not apply to all resource types.
       * @example 50
       */
      diskSpaceBilledGb?: number;
      /**
       * @description The amount charged for high performance storage for the billing period, in USD.
       * @example 100.5
       */
      highPerformanceStorageAmount?: number;
      /**
       * @description The amount of high performance storage disk space billed for the billing period, in gigabytes (GB).
       * @example 50
       */
      highPerformanceStorageDiskSpaceBilledGb?: number;
      /**
       * Format: date-time
       * @description The start of the period for which the billing record applies.
       * @example 2023-01-01T00:00:00Z
       */
      time?: string;
    }[];
  };
  responses: never;
  parameters: never;
  requestBodies: never;
  headers: never;
  pathItems: never;
}
export type $defs = Record<string, never>;
export interface operations {
  GetOpenAPI: {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description The openapi.json specification. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": Record<string, never>;
        };
      };
    };
  };
  GetDocs: {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description The docs UI for the OpenAPI schema. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "text/html": string;
        };
      };
      /** @description Bad Request - the request could not be processed. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  ListPods: {
    parameters: {
      query?: {
        computeType?: "GPU" | "CPU";
        cpuFlavorId?: string[];
        dataCenterId?: string[];
        desiredStatus?: "RUNNING" | "EXITED" | "TERMINATED";
        endpointId?: string;
        gpuTypeId?: string[];
        id?: string;
        imageName?: string;
        includeMachine?: boolean;
        includeNetworkVolume?: boolean;
        includeSavingsPlans?: boolean;
        includeTemplate?: boolean;
        includeWorkers?: boolean;
        name?: string;
        networkVolumeId?: string;
        templateId?: string;
      };
      header?: never;
      path?: never;
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful operation. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["Pods"];
        };
      };
      /** @description Invalid ID supplied. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Pod not found. */
      404: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  CreatePod: {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /** @description Input for Pod creation. */
    requestBody: {
      content: {
        "application/json": components["schemas"]["PodCreateInput"];
      };
    };
    responses: {
      /** @description Pod successfully created. */
      201: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["Pod"];
        };
      };
      /** @description Invalid input. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  GetPod: {
    parameters: {
      query?: {
        includeMachine?: boolean;
        includeNetworkVolume?: boolean;
        includeSavingsPlans?: boolean;
        includeTemplate?: boolean;
        includeWorkers?: boolean;
      };
      header?: never;
      path: {
        /** @description ID of Pod to return. */
        podId: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful operation. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["Pod"];
        };
      };
      /** @description Invalid ID supplied. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Pod not found. */
      404: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  DeletePod: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        /** @description Pod ID to delete. */
        podId: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Pod successfully deleted. */
      204: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Invalid Pod ID. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Unauthorized. */
      401: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  UpdatePod: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        /** @description ID of Pod that needs to be updated. */
        podId: string;
      };
      cookie?: never;
    };
    /** @description Form data to update a Pod. */
    requestBody: {
      content: {
        "application/json": components["schemas"]["PodUpdateInput"];
      };
    };
    responses: {
      /** @description Pod successfully updated. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["Pod"];
        };
      };
      /** @description Invalid input. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  UpdatePod: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        /** @description ID of Pod that needs to be updated. */
        podId: string;
      };
      cookie?: never;
    };
    /** @description Form data to update a Pod. */
    requestBody: {
      content: {
        "application/json": components["schemas"]["PodUpdateInput"];
      };
    };
    responses: {
      /** @description Pod successfully updated. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["Pod"];
        };
      };
      /** @description Invalid input. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  StartPod: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        /** @description Pod ID to start. */
        podId: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Pod successfully started. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Invalid Pod ID. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Unauthorized. */
      401: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  StopPod: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        /** @description Pod ID to stop. */
        podId: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Pod successfully stopped. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Invalid Pod ID. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Unauthorized. */
      401: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  ResetPod: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        /** @description Pod ID to reset. */
        podId: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Pod successfully reset. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Invalid Pod ID. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Unauthorized. */
      401: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  RestartPod: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        /** @description Pod ID to restart. */
        podId: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Invalid Pod ID. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Unauthorized. */
      401: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  ListEndpoints: {
    parameters: {
      query?: {
        includeTemplate?: boolean;
        includeWorkers?: boolean;
      };
      header?: never;
      path?: never;
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful operation. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["Endpoints"];
        };
      };
      /** @description Invalid ID supplied. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Endpoint not found. */
      404: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  CreateEndpoint: {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /** @description Create a new endpoint. */
    requestBody: {
      content: {
        "application/json": components["schemas"]["EndpointCreateInput"];
      };
    };
    responses: {
      /** @description Successful operation. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["Endpoint"];
        };
      };
      /** @description Invalid input. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  GetEndpoint: {
    parameters: {
      query?: {
        includeTemplate?: boolean;
        includeWorkers?: boolean;
      };
      header?: never;
      path: {
        /** @description ID of endpoint to return. */
        endpointId: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful operation. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["Endpoint"];
        };
      };
      /** @description Invalid ID supplied. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Endpoint not found. */
      404: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  DeleteEndpoint: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        /** @description Endpoint ID to delete. */
        endpointId: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Endpoint successfully deleted. */
      204: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Invalid endpoint ID. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Unauthorized. */
      401: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  UpdateEndpoint: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        /** @description ID of endpoint that needs to be updated. */
        endpointId: string;
      };
      cookie?: never;
    };
    /** @description Update an endpoint. */
    requestBody: {
      content: {
        "application/json": components["schemas"]["EndpointUpdateInput"];
      };
    };
    responses: {
      /** @description Successful operation. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["Endpoint"];
        };
      };
      /** @description Invalid input. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  UpdateEndpoint: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        /** @description ID of endpoint that needs to be updated. */
        endpointId: string;
      };
      cookie?: never;
    };
    /** @description Update an endpoint. */
    requestBody: {
      content: {
        "application/json": components["schemas"]["EndpointUpdateInput"];
      };
    };
    responses: {
      /** @description Successful operation. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["Endpoint"];
        };
      };
      /** @description Invalid input. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  ListTemplates: {
    parameters: {
      query?: {
        includeEndpointBoundTemplates?: boolean;
        includePublicTemplates?: boolean;
        includeRunpodTemplates?: boolean;
      };
      header?: never;
      path?: never;
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful operation. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["Templates"];
        };
      };
      /** @description Invalid ID supplied. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Template not found. */
      404: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  CreateTemplate: {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /** @description Create a new template. */
    requestBody: {
      content: {
        "application/json": components["schemas"]["TemplateCreateInput"];
      };
    };
    responses: {
      /** @description Successful operation. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["Template"];
        };
      };
      /** @description Invalid input. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  GetTemplate: {
    parameters: {
      query?: {
        includeEndpointBoundTemplates?: boolean;
        includePublicTemplates?: boolean;
        includeRunpodTemplates?: boolean;
      };
      header?: never;
      path: {
        /** @description ID of template to return. */
        templateId: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful operation. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["Template"];
        };
      };
      /** @description Invalid ID supplied. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Template not found. */
      404: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  DeleteTemplate: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        /** @description Template ID to delete. */
        templateId: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Template successfully deleted. */
      204: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Invalid template ID. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Unauthorized. */
      401: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  UpdateTemplate: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        /** @description ID of template that needs to be updated. */
        templateId: string;
      };
      cookie?: never;
    };
    /** @description Update a template. */
    requestBody: {
      content: {
        "application/json": components["schemas"]["TemplateUpdateInput"];
      };
    };
    responses: {
      /** @description Successful operation. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["Template"];
        };
      };
      /** @description Invalid input. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  UpdateTemplate: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        /** @description ID of template that needs to be updated. */
        templateId: string;
      };
      cookie?: never;
    };
    /** @description Update a template. */
    requestBody: {
      content: {
        "application/json": components["schemas"]["TemplateUpdateInput"];
      };
    };
    responses: {
      /** @description Successful operation. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["Template"];
        };
      };
      /** @description Invalid input. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  ListNetworkVolumes: {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful operation. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["NetworkVolumes"];
        };
      };
      /** @description Invalid ID supplied. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Network volume not found. */
      404: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  CreateNetworkVolume: {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /** @description Create a new network volume. */
    requestBody: {
      content: {
        "application/json": components["schemas"]["NetworkVolumeCreateInput"];
      };
    };
    responses: {
      /** @description Successful operation. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["NetworkVolume"];
        };
      };
      /** @description Invalid input. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  GetNetworkVolume: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        /** @description ID of network volume to return. */
        networkVolumeId: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful operation. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["NetworkVolume"];
        };
      };
      /** @description Invalid ID supplied. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Network volume not found. */
      404: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  DeleteNetworkVolume: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        /** @description Network volume ID to delete. */
        networkVolumeId: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Network volume successfully deleted. */
      204: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Invalid network volume ID. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Unauthorized. */
      401: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  UpdateNetworkVolume: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        /** @description ID of network volume that needs to be updated. */
        networkVolumeId: string;
      };
      cookie?: never;
    };
    /** @description Update a network volume. */
    requestBody: {
      content: {
        "application/json": components["schemas"]["NetworkVolumeUpdateInput"];
      };
    };
    responses: {
      /** @description Successful operation. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["NetworkVolume"];
        };
      };
      /** @description Invalid input. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  UpdateNetworkVolume: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        /** @description ID of network volume that needs to be updated. */
        networkVolumeId: string;
      };
      cookie?: never;
    };
    /** @description Update a network volume. */
    requestBody: {
      content: {
        "application/json": components["schemas"]["NetworkVolumeUpdateInput"];
      };
    };
    responses: {
      /** @description Successful operation. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["NetworkVolume"];
        };
      };
      /** @description Invalid input. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  ListContainerRegistryAuths: {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful operation. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["ContainerRegistryAuths"];
        };
      };
      /** @description Invalid ID supplied. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Container registry auth not found. */
      404: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  CreateContainerRegistryAuth: {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /** @description Create a new container registry auth. */
    requestBody: {
      content: {
        "application/json": components["schemas"]["ContainerRegistryAuthCreateInput"];
      };
    };
    responses: {
      /** @description Successful operation. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["ContainerRegistryAuth"];
        };
      };
      /** @description Invalid input. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  GetContainerRegistryAuth: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        /** @description ID of container registry auth to return. */
        containerRegistryAuthId: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful operation. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["ContainerRegistryAuth"];
        };
      };
      /** @description Invalid ID supplied. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Container registry auth not found. */
      404: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  DeleteContainerRegistryAuth: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        /** @description Container registry auth ID to delete. */
        containerRegistryAuthId: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Container registry auth successfully deleted. */
      204: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Invalid container registry auth ID. */
      400: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Unauthorized. */
      401: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
    };
  };
  PodBilling: {
    parameters: {
      query?: {
        bucketSize?: "hour" | "day" | "week" | "month" | "year";
        endTime?: string;
        gpuTypeId?:
          | "NVIDIA GeForce RTX 4090"
          | "NVIDIA A40"
          | "NVIDIA RTX A5000"
          | "NVIDIA GeForce RTX 5090"
          | "NVIDIA H100 80GB HBM3"
          | "NVIDIA GeForce RTX 3090"
          | "NVIDIA RTX A4500"
          | "NVIDIA L40S"
          | "NVIDIA H200"
          | "NVIDIA L4"
          | "NVIDIA RTX 6000 Ada Generation"
          | "NVIDIA A100-SXM4-80GB"
          | "NVIDIA RTX 4000 Ada Generation"
          | "NVIDIA RTX A6000"
          | "NVIDIA A100 80GB PCIe"
          | "NVIDIA RTX 2000 Ada Generation"
          | "NVIDIA RTX A4000"
          | "NVIDIA RTX PRO 6000 Blackwell Server Edition"
          | "NVIDIA H100 PCIe"
          | "NVIDIA H100 NVL"
          | "NVIDIA L40"
          | "NVIDIA B200"
          | "NVIDIA GeForce RTX 3080 Ti"
          | "NVIDIA RTX PRO 6000 Blackwell Workstation Edition"
          | "NVIDIA GeForce RTX 3080"
          | "NVIDIA GeForce RTX 3070"
          | "AMD Instinct MI300X OAM"
          | "NVIDIA GeForce RTX 4080 SUPER"
          | "Tesla V100-PCIE-16GB"
          | "Tesla V100-SXM2-32GB"
          | "NVIDIA RTX 5000 Ada Generation"
          | "NVIDIA GeForce RTX 4070 Ti"
          | "NVIDIA RTX 4000 SFF Ada Generation"
          | "NVIDIA GeForce RTX 3090 Ti"
          | "NVIDIA RTX A2000"
          | "NVIDIA GeForce RTX 4080"
          | "NVIDIA A30"
          | "NVIDIA GeForce RTX 5080"
          | "Tesla V100-FHHL-16GB"
          | "NVIDIA H200 NVL"
          | "Tesla V100-SXM2-16GB"
          | "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition"
          | "NVIDIA A5000 Ada"
          | "Tesla V100-PCIE-32GB"
          | "NVIDIA  RTX A4500"
          | "NVIDIA  A30"
          | "NVIDIA GeForce RTX 3080TI"
          | "Tesla T4"
          | "NVIDIA RTX A30";
        grouping?: "podId" | "gpuTypeId";
        podId?: string;
        startTime?: string;
      };
      header?: never;
      path?: never;
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful operation. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["BillingRecords"];
        };
      };
    };
  };
  EndpointBilling: {
    parameters: {
      query?: {
        bucketSize?: "hour" | "day" | "week" | "month" | "year";
        dataCenterId?: (
          | "EU-RO-1"
          | "CA-MTL-1"
          | "EU-SE-1"
          | "US-IL-1"
          | "EUR-IS-1"
          | "EU-CZ-1"
          | "US-TX-3"
          | "EUR-IS-2"
          | "US-KS-2"
          | "US-GA-2"
          | "US-WA-1"
          | "US-TX-1"
          | "CA-MTL-3"
          | "EU-NL-1"
          | "US-TX-4"
          | "US-CA-2"
          | "US-NC-1"
          | "OC-AU-1"
          | "US-DE-1"
          | "EUR-IS-3"
          | "CA-MTL-2"
          | "AP-JP-1"
          | "EUR-NO-1"
          | "EU-FR-1"
          | "US-KS-3"
          | "US-GA-1"
        )[];
        endpointId?: string;
        endTime?: string;
        gpuTypeId?: (
          | "NVIDIA GeForce RTX 4090"
          | "NVIDIA A40"
          | "NVIDIA RTX A5000"
          | "NVIDIA GeForce RTX 5090"
          | "NVIDIA H100 80GB HBM3"
          | "NVIDIA GeForce RTX 3090"
          | "NVIDIA RTX A4500"
          | "NVIDIA L40S"
          | "NVIDIA H200"
          | "NVIDIA L4"
          | "NVIDIA RTX 6000 Ada Generation"
          | "NVIDIA A100-SXM4-80GB"
          | "NVIDIA RTX 4000 Ada Generation"
          | "NVIDIA RTX A6000"
          | "NVIDIA A100 80GB PCIe"
          | "NVIDIA RTX 2000 Ada Generation"
          | "NVIDIA RTX A4000"
          | "NVIDIA RTX PRO 6000 Blackwell Server Edition"
          | "NVIDIA H100 PCIe"
          | "NVIDIA H100 NVL"
          | "NVIDIA L40"
          | "NVIDIA B200"
          | "NVIDIA GeForce RTX 3080 Ti"
          | "NVIDIA RTX PRO 6000 Blackwell Workstation Edition"
          | "NVIDIA GeForce RTX 3080"
          | "NVIDIA GeForce RTX 3070"
          | "AMD Instinct MI300X OAM"
          | "NVIDIA GeForce RTX 4080 SUPER"
          | "Tesla V100-PCIE-16GB"
          | "Tesla V100-SXM2-32GB"
          | "NVIDIA RTX 5000 Ada Generation"
          | "NVIDIA GeForce RTX 4070 Ti"
          | "NVIDIA RTX 4000 SFF Ada Generation"
          | "NVIDIA GeForce RTX 3090 Ti"
          | "NVIDIA RTX A2000"
          | "NVIDIA GeForce RTX 4080"
          | "NVIDIA A30"
          | "NVIDIA GeForce RTX 5080"
          | "Tesla V100-FHHL-16GB"
          | "NVIDIA H200 NVL"
          | "Tesla V100-SXM2-16GB"
          | "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition"
          | "NVIDIA A5000 Ada"
          | "Tesla V100-PCIE-32GB"
          | "NVIDIA  RTX A4500"
          | "NVIDIA  A30"
          | "NVIDIA GeForce RTX 3080TI"
          | "Tesla T4"
          | "NVIDIA RTX A30"
        )[];
        grouping?: "endpointId" | "podId" | "gpuTypeId";
        imageName?: string;
        startTime?: string;
        templateId?: string;
      };
      header?: never;
      path?: never;
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful operation. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["BillingRecords"];
        };
      };
    };
  };
  NetworkVolumeBilling: {
    parameters: {
      query?: {
        bucketSize?: "hour" | "day" | "week" | "month" | "year";
        endTime?: string;
        startTime?: string;
      };
      header?: never;
      path?: never;
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful operation. */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          "application/json": components["schemas"]["NetworkVolumeBillingRecords"];
        };
      };
    };
  };
}
